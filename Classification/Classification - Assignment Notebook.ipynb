{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this is a template notebook for Assignment 2 on Clustering. To get a 60 you will need to complete chapter 1 and 2.\n",
    "    The template is also just an indication. You can add more cells if needed, and can of course delete this line**\n",
    "\n",
    "# <span style ='background:yellow'>Assignment 5\n",
    "Author: <span style='background:yellow'>Mohaymen Sameen</span><br>\n",
    "Student number: <span style='background:yellow'>627650</span><br>\n",
    "Date: <span style='background:yellow'>23-05-2021</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook to work you must have installed the following packages (usually via pip install *packageName*:\n",
    "* numpy\n",
    "* pandas\n",
    "* **Sklearn and others**\n",
    "\n",
    "From these we will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: decision-tree-id3 in /opt/anaconda3/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /opt/anaconda3/lib/python3.8/site-packages (from decision-tree-id3) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from decision-tree-id3) (1.19.2)\n",
      "Requirement already satisfied: nose>=1.1.2 in /opt/anaconda3/lib/python3.8/site-packages (from decision-tree-id3) (1.3.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.17->decision-tree-id3) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.17->decision-tree-id3) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.17->decision-tree-id3) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "# enter here all those 'from .... import ....'\n",
    "!pip install decision-tree-id3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "from ipywidgets import interact, fixed\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Data\n",
    "We are going to use the datafile **<span style ='background:yellow'>Stars.csv</span>**. This contains data from **<span style ='background:yellow'>differet stars and can possibly predict the star type</span>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading stars.csv\n",
    "my_data = pd.read_csv(\"Stars.csv\", sep=',', skipinitialspace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>L</th>\n",
       "      <th>R</th>\n",
       "      <th>A_M</th>\n",
       "      <th>Color</th>\n",
       "      <th>Spectral_Class</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3068</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>16.12</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3042</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.1542</td>\n",
       "      <td>16.60</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>18.70</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>16.65</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>20.06</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature         L       R    A_M Color Spectral_Class  Type\n",
       "0         3068  0.002400  0.1700  16.12   Red              M     0\n",
       "1         3042  0.000500  0.1542  16.60   Red              M     0\n",
       "2         2600  0.000300  0.1020  18.70   Red              M     0\n",
       "3         2800  0.000200  0.1600  16.65   Red              M     0\n",
       "4         1939  0.000138  0.1030  20.06   Red              M     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying first 5 rows\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>**ATTRIBUTES:**\n",
    "**Temperature** -- K,\n",
    "**L** -- L/Lo (Luminosity),\n",
    "**R** -- R/Ro (Radius),\n",
    "**AM** -- Mv (Absolute Magnitude),\n",
    "**Color** -- General Color of Spectrum,\n",
    "**Spectral_Class** -- O,B,A,F,G,K,M / SMASS - https://lweb.cfa.harvard.edu/~pberlind/atlas/htmls/note.html#:~:text=The%20spectral%20types%20and%20sub,%22color%22%20and%20surface%20brightness.,\n",
    "**Type** -- Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants\n",
    "\n",
    "**Type**\n",
    "from 0 to 5\n",
    "\n",
    "Red Dwarf - 0\n",
    "Brown Dwarf - 1\n",
    "White Dwarf - 2\n",
    "Main Sequence - 3\n",
    "Super Giants - 4\n",
    "Hyper Giants - 5\n",
    "\n",
    "Variables:\n",
    "**Type** (The outcome variable 'y')\n",
    "**L (Luminosity), R(Radius)** (predictor variables 'x')</span>\n",
    "\n",
    "*note:* Its easiest if your independent x variables are numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical dependent variable <span style ='background:yellow'>Type</span> has the following categories: **Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5], dtype='int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different Categories for the Types (Y-variable)\n",
    "my_data[\"Type\"]=pd.Categorical(my_data[\"Type\"])\n",
    "my_data[\"Type\"].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need some training and testing data, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split data in training and testing\n",
    "x = np.array(my_data[\"L\"])\n",
    "y = np.array(my_data[\"Type\"])\n",
    "xRes = x.reshape((-1,1))\n",
    "yRes = y.reshape((-1,1))\n",
    "\n",
    "xRes_train, xRes_test, yRes_train, yRes_test = train_test_split(xRes, yRes, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set, lets try to predict this using our independent variable **<span style ='background:yellow'>Luminosity</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Basic Classification Models\n",
    "\n",
    "In the Jupyter Notebook from lecture 5 a few different Clustering techniques were discussed. Lets explore how these perform on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at some 'real' models, its a good idea to get a baseline in by using one or more of the dummy classifiers. Lets see how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10416666666666667\n",
      "[[ 0  0  0  0 20  0]\n",
      " [ 0  0  0  0 12  0]\n",
      " [ 0  0  0  0 14  0]\n",
      " [ 0  0  0  0 21  0]\n",
      " [ 0  0  0  0 10  0]\n",
      " [ 0  0  0  0 19  0]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier - Most Frequent\n",
    "\n",
    "dumMF = DummyClassifier(strategy='most_frequent')\n",
    "dumMF = dumMF.fit(xRes_train, yRes_train)\n",
    "y_pred = dumMF.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13541666666666666\n",
      "[[2 4 8 3 2 1]\n",
      " [1 3 3 2 1 2]\n",
      " [3 2 3 2 3 1]\n",
      " [0 3 7 2 3 6]\n",
      " [3 2 0 2 1 2]\n",
      " [0 2 6 5 4 2]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier - Stratified\n",
    "dumSrat = DummyClassifier(strategy=\"stratified\")\n",
    "dumSrat = dumSrat.fit(xRes_train, yRes_train)\n",
    "y_pred = dumSrat.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10416666666666667\n",
      "[[ 0  0  0  0 20  0]\n",
      " [ 0  0  0  0 12  0]\n",
      " [ 0  0  0  0 14  0]\n",
      " [ 0  0  0  0 21  0]\n",
      " [ 0  0  0  0 10  0]\n",
      " [ 0  0  0  0 19  0]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier - Prior\n",
    "dumPrior = DummyClassifier(strategy=\"prior\")\n",
    "dumPrior.fit(xRes_train, yRes_train)\n",
    "y_pred = dumPrior.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14583333333333334\n",
      "[[4 3 1 3 5 4]\n",
      " [3 1 0 6 1 1]\n",
      " [3 2 1 1 6 1]\n",
      " [5 2 2 4 6 2]\n",
      " [0 3 2 4 0 1]\n",
      " [4 1 4 5 1 4]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier - Uniform\n",
    "dumUni = DummyClassifier(strategy=\"uniform\")\n",
    "dumUni.fit(xRes_train, yRes_train)\n",
    "y_pred = dumUni.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>There are 4 classifier types. The first one is most frequent. This produces the most frequent label in the training set. The second one is stratified which generates predictions by respecting the training set's class distribution. The third one is prior which always predicts the class that maximizes the class prior. And finally the last one is uniform. This method generates predictions uniformly at random. For each dummy classifier type, there are accuracy scores to show how accurate the prediction is and the confusion matrixes displays the performance of each classifier. Prior has the same results as most frequent as it looks at the prior results. The \"most frequent's\" confusion matrix displays the prediction of the type of star using luminosity.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, those are our 'baseline'. A model should be able to at least outperform these.\n",
    "\n",
    "Lets dive in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Naive Bayes\n",
    "\n",
    "The first model discussed was the Naive Bayes model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>Naive Bayes is a supervised learning algorithm based on applying the Bayes’ theorem with the assumption of independency between the features or class variables. It works by using Bayes' theorem and is useful for calculating condional probabilities. The formula is: P(A|B)= (P(B|A) x P(A))/P(B), using this formula you can calculate the probability of A based on the probability of B</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create and fit this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our model to this GaussianNB\n",
    "gaussNB = GaussianNB()\n",
    "\n",
    "#Getting the specific fields to use\n",
    "mDat =my_data[[\"L\", \"R\", \"Type\"]]\n",
    "\n",
    "# Setting input and output\n",
    "X = mDat[[\"L\", \"R\"]]\n",
    "y = mDat[\"Type\"]\n",
    "\n",
    "#Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=109)\n",
    "\n",
    "\n",
    "#Creating the model and prediction\n",
    "gaussNB.fit(X_train, y_train)\n",
    "y_pred = gaussNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to measure its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  2  0  4  0  0]\n",
      " [ 0  0  0  1  6  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score and confusion matrix\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>We got a 56% accuracy rating which shows the accuracy of our predictions on our new model. The numbers which you see on the confusion matrix show the prediction of the luminosity and radius and how many times they were correctly predicted and the ones which predicted it wrong. As you can see, this is a remarkable difference compared to the dummy classifiers. We were mostly getting around the 10% to 20% range most of the time. But now using the naive bayes method, our model got a much more accurate prediction of 56%.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also have a look at what a prediction would be. If the **<span style ='background:yellow'>Luminosity<span>** has a score of **<span style ='background:yellow'>9</span>** and the **<span style ='background:yellow'>Radius</span>** has a score of **<span style ='background:yellow'>12</span>**, then this model will predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction \n",
    "(9+12)/(9+5+2+12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about it for NB. A nice thing about NB is that it doesn't really require any parameters. Lets look at our next technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines\n",
    "The second model discussed were Support Vector Machines. There is a plural here, because we can use different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>Support vector machines is also another supervised machine learning algorithm that analyses data for predictions for classification. It works by using classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text. </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic kernel is the linear one, so we'll attempt that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model, and fit the data.\n",
    "svmLin = SVC(kernel='linear')\n",
    "svmLin.fit(X_train, y_train)\n",
    "y_pred = svmLin.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring its performance...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "[[ 7  0  2  0  0  0]\n",
      " [ 2  3  0  0  0  0]\n",
      " [ 0  0  9  0  0  0]\n",
      " [ 0  0  0  5  1  0]\n",
      " [ 0  0  0  1  6  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, and the NB?>You might have noticed that the accuracy score is way higher than those of the dummy classifiers as well as the Naive bayes. The dummy classifiers were in the range of 10% to 20%, the naive bayes was 56%. Now the the support vector machine model gives us an accuracy score of 88%, much higher than the rest of them. The numbers you see in the confusion matrix tell you the diagonals and how much seperation there is between them. You can probably see that one diagonal has most of the scores/counts while the others have almost zero counts or scores.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same for the other kernels that were discussed, i.e. rbf, polynomial, and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF results\n",
      "accuracy score:  0.22916666666666666\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  1  0]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  0 12  0]]\n"
     ]
    }
   ],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "\n",
    "#rbf\n",
    "rbfKer = SVC(kernel='rbf')\n",
    "rbfKer.fit(X_train, y_train)\n",
    "y_pred = rbfKer.predict(X_test)\n",
    "\n",
    "print('RBF results')\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial results\n",
      "accuracy score:  0.22916666666666666\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  1  0]\n",
      " [ 0  1  0  0  6  0]\n",
      " [ 0  2  0  0 10  0]]\n"
     ]
    }
   ],
   "source": [
    "#Polynomial Kernel\n",
    "svmPol = SVC(kernel='poly')\n",
    "svmPol.fit(X_train, y_train)\n",
    "y_pred = svmPol.predict(X_test)\n",
    "\n",
    "print('Polynomial results')\n",
    "print('accuracy score: ',metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.2916666666666667\n",
      "[[0 9 0 0 0 0]\n",
      " [0 5 0 0 0 0]\n",
      " [0 9 0 0 0 0]\n",
      " [0 5 0 0 0 1]\n",
      " [0 0 0 0 4 3]\n",
      " [0 0 0 0 7 5]]\n"
     ]
    }
   ],
   "source": [
    "#Sigmoid Kernel\n",
    "svmSigm = SVC(kernel='sigmoid')\n",
    "svmSigm.fit(X_train, y_train)\n",
    "y_pred = svmSigm.predict(X_test)\n",
    "\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>We have now used 4 types of support vector machines, linear, RBF aka(Radial Basis Function), polynomial and sigmoid. Out all of them, linear worked best, which means that the data is mostly split in sections where it can be cut by diagonals. RBF and polynomial have similar results of 23% and sigmoid being 29%. All of these methods still perform much better than the dummy classifiers.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, lets move on to the third technique..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. K-Nearest Neighbors\n",
    "The third technique is the K-Nearest Neighbors (KNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. The KNN algorithm works by assuming that similar things exist in close proximity. In other words, similar things are near to each other.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this we need to do some additional steps.\n",
    "\n",
    "First we need to normalize our x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the StandardScaler to normalize the two x variables\n",
    "\n",
    "# setting the scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Convert the train and test X values, using the same scaler (so based on the X_train)\n",
    "X_trainScaled = scaler.transform(X_train)\n",
    "X_testScaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we need to determine how many neighbors (k) we want. To do this we'll visualize the results using different values for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error Rate')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code to create the graph with Error Rate vs. K-values.\n",
    "\n",
    "#I got 14 by checking the shape for the X_train set to the power of 0.5\n",
    "knn = KNeighborsClassifier(n_neighbors=14)\n",
    "\n",
    "error_rate=[]\n",
    "for i in range(1,80):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_trainScaled, y_train)\n",
    "    pred_i = knn.predict(X_testScaled)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,80),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>As you can probably see from the graph, the error rate is low at around 0.1-0.15 and the k-range from 0 to 43. After 43 K, the error rate starts to spike up to about 0.78 error rate. We can choose K = 7 since it is the first K with the lowest error rate</span>**\n",
    "\n",
    "*Note:* +0.5 if you also use the GridSearch technique to decide on k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best p: 1\n",
      "Best n_neighbors: 1\n"
     ]
    }
   ],
   "source": [
    "#Lets use a grid search to decide on K as well.\n",
    "\n",
    "## List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "##Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "##Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "##Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "##Fit the model\n",
    "best_model = clf.fit(X_trainScaled, y_train)\n",
    "##Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we want k to be, we can create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model with the selected k\n",
    "# The result for the gridsearch technique was a leaf size of 1, p of 1 and 1 neighbors\n",
    "knnOptimal = KNeighborsClassifier(n_neighbors=1, leaf_size=1, p=1)\n",
    "y_pred=knn.predict(X_testScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out how good it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22916666666666666\n",
      "[[9 0 0 0 0 0]\n",
      " [4 1 0 0 0 0]\n",
      " [9 0 0 0 0 0]\n",
      " [0 5 0 1 0 0]\n",
      " [0 0 0 7 0 0]\n",
      " [0 0 0 4 8 0]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>After selecting our k nearest neighbor of 1 and fitting it into our model,we got an accuracy score of 23 percent. It is still higher than all the dummy classifiers and the sigmoid. But the result is exactly the same as the RBF and polynomial support vectors. Also it performs worse than the naive bayes which had an accuracy score of 56%.  </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more basic technique to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Decision Trees\n",
    "The last technique that was discussed in detail, were the Decision Trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>Decision trees is a supervised machine learning algorithm that can be used for classification or regression methods. Decision trees use multiple algorithms to decide to split a node into two or more other sub-nodes. The creation of these sub-nodes increases the Uniformity of the resultant sub-nodes.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variations were discussed:\n",
    "\n",
    "* ID3 (or entropy with sklearn)\n",
    "* Gini\n",
    "* Random Forest\n",
    "* Extra trees\n",
    "\n",
    "Hopefully we have the hang of this now, so lets do each of them in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5208333333333334\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0  9  0  0  0]\n",
      " [ 0  0  0  6  0  0]\n",
      " [ 0  0  0  2  5  0]\n",
      " [ 0  0  0  0 12  0]]\n"
     ]
    }
   ],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "\n",
    "#importing six and sys\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "# then finally id3\n",
    "from id3 import Id3Estimator\n",
    "\n",
    "\n",
    "#ID3 \n",
    "id3_dtc=Id3Estimator()\n",
    "id3_dtc.fit(X_trainScaled,y_train)\n",
    "y_pred = id3_dtc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8958333333333334\n",
      "[[ 7  2  0  0  0  0]\n",
      " [ 1  4  0  0  0  0]\n",
      " [ 0  0  9  0  0  0]\n",
      " [ 0  0  0  6  0  0]\n",
      " [ 0  0  0  2  5  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "#Gini\n",
    "gini_dtc = DecisionTreeClassifier(criterion = \"gini\")\n",
    "gini_dtc.fit(X_trainScaled,y_train)\n",
    "y_pred = gini_dtc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "[[ 7  2  0  0  0  0]\n",
      " [ 1  4  0  0  0  0]\n",
      " [ 0  0  9  0  0  0]\n",
      " [ 0  0  0  6  0  0]\n",
      " [ 0  0  0  1  6  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfcModel = rfc.fit(X_trainScaled, y_train)\n",
    "y_pred = rfcModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9583333333333334\n",
      "[[ 7  2  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0  9  0  0  0]\n",
      " [ 0  0  0  6  0  0]\n",
      " [ 0  0  0  0  7  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "#Extra trees\n",
    "erfc = ExtraTreesClassifier(random_state=0)\n",
    "erfc = erfc.fit(X_trainScaled, y_train)\n",
    "y_pred = erfc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>Alright so after having a look at the different variations of decision trees, we can clearly see an incredibly high accuracy score for most of the variations. The worst one is ID3 scoring a 52%. As we move on to Gini, Random Forest and Extra trees, we start to get really high accuracy scores. For Gini we get an accuracy score of 90%. For Random forest, we get an accuracy score of 92% and finally for the last variation which is also the best performing model so far is the extra trees variation scoring a 96%. For gini, random forest and extra trees, the accuracy score outperforms every other classification algorthim so far.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last set of techniques to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Other Models\n",
    "In the Jupyter Notebook from the lecture, in chapter 5.4 a few more techniques were simply shown:\n",
    "\n",
    "* Linear Discriminant Analysis\n",
    "* Quadratic Discriminant Analysis\n",
    "* Logistic Regression Classifier\n",
    "* Multinomial Logistic Regression Classification\n",
    "* Adaptive Boosting\n",
    "* Gradient Boosting\n",
    "* Histogram Gradient Boosting\n",
    "* XGBoost\n",
    "* Stacking\n",
    "\n",
    "Out of curiousity lets see how these perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score (the confusion matrix is here optional).\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "#Linear Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "ldaModel=lda.fit(X_trainScaled, y_train)\n",
    "y_pred=ldaModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quadratic Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qdaModel=qda.fit(X_trainScaled, y_train)\n",
    "y_pred=ldaModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "lrModel = logreg.fit(X_trainScaled, y_train)\n",
    "y_pred = lrModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Logistic Regression Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(multi_class='multinomial')\n",
    "lrModel = logreg.fit(X_trainScaled, y_train)\n",
    "y_pred = lrModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaptive Boosting\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaBst = AdaBoostClassifier(random_state=0)\n",
    "adaBst = adaBst.fit(X_trainScaled, y_train)\n",
    "y_pred = adaBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gradBst = GradientBoostingClassifier(random_state=0)\n",
    "gradBst = gradBst.fit(X_trainScaled, y_train)\n",
    "y_pred = gradBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram Gradient Boosting\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "histBst = HistGradientBoostingClassifier(random_state=0)\n",
    "histBst = histBst.fit(X_trainScaled, y_train)\n",
    "y_pred = histBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_trainScaled, y_train)\n",
    "y_pred = xgb.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                          LinearSVC(random_state=42)))]\n",
    "\n",
    "stackCl = StackingClassifier(estimators=estimators, final_estimator = LogisticRegression())\n",
    "stackCl.fit(X_trainScaled, y_train)\n",
    "y_pred = stackCl.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>After trying other models and techniques, we also got some pretty impressive results. From all of the new techniques used, the technique with the highest accuracy score was Histogram Gradient Boosting. There were some that also had the same accuracy scores such as Stacking and Gradient boosting that scored an 90%.Linear Discriminant Analysis, Quadratic Discriminant Analysis, Logistic Regression Classifier and Multinomial Logistic Regression Classification had the same score of 48% </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center>-----Chapters 1 and 2 are required to be fully completed to get a 60, the next few chapters will give a +10 for each chapter.<br> \n",
    "    However the template is not as extensive as the previous chapters. <br>\n",
    "    You can select any chapter below the order is not fixed (you can leave the others empty)<br>\n",
    "    You don't have to use the same dataset for the chapters below. If it helps in clarification you can use another dataset, but then make sure to include it as you submit.\n",
    "    ----</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualisation\n",
    "\n",
    "With two input parameters we can actually determine visually where a model will classify a variable into which category. An overview of such plots is shown at https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "We cannot copy that code since it does a comparison. What we want is a function that takes the X and Y data as input, as well as the model to be used and then shows the decision areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Categorical Input\n",
    "With classification we have a categorical output variable, but what if we also have one or more categorical input variables.\n",
    "\n",
    "One popular technique is one-hot-encoding, but there are others.\n",
    "\n",
    "In this chapter we'll discuss **<span style ='background:yellow'>\\<your chosen technique></span>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the technique in detail. What does it do and how does it work></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code of using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Performance\n",
    "Some models get higher accuracy scores than others. In the Jupyter Notebook from the lecture the UFC data was used and the QDA had the highest accuracy score: 0.6747. The big question is, can it be done better? First areas to look for improvement are to simply increase the number of input variables, or tweak some parameters of some of the models, or a combination of both.\n",
    "\n",
    "In this chapter we'll give it an attempt.\n",
    "\n",
    "First we need to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load the UFC data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain your attempt, what did you do.></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that will generate an accuracy score for the outcome that is higher than 0.6747\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. A New Technique\n",
    "\n",
    "Many techniques were discussed in class and the lecture Jupyter Notebook, but there are a lot more. In this chapter the \\<your chosen new technique> is discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain in detail this new technique. Note that other students should be able to understand it from your explanation alone!></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code on using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<feel free to use more cells for this, you probably need them></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
