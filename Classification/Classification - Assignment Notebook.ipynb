{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this is a template notebook for Assignment 2 on Clustering. To get a 60 you will need to complete chapter 1 and 2.\n",
    "    The template is also just an indication. You can add more cells if needed, and can of course delete this line**\n",
    "\n",
    "# <span style ='background:yellow'>Assignment 5\n",
    "Author: <span style='background:yellow'>Mohaymen Sameen</span><br>\n",
    "Student number: <span style='background:yellow'>627650</span><br>\n",
    "Date: <span style='background:yellow'>23-05-2021</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook to work you must have installed the following packages (usually via pip install *packageName*:\n",
    "* numpy\n",
    "* pandas\n",
    "* **\\<add other packages\\>**\n",
    "\n",
    "From these we will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter here all those 'from .... import ....'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Data\n",
    "We are going to use the datafile **<span style ='background:yellow'>Stars.csv</span>**. This contains data from **<span style ='background:yellow'>differet stars and can possibly predict the star type</span>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading stars.csv\n",
    "my_data = pd.read_csv(\"Stars.csv\", sep=',', skipinitialspace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>L</th>\n",
       "      <th>R</th>\n",
       "      <th>A_M</th>\n",
       "      <th>Color</th>\n",
       "      <th>Spectral_Class</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3068</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>16.12</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3042</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.1542</td>\n",
       "      <td>16.60</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>18.70</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>16.65</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1939</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>20.06</td>\n",
       "      <td>Red</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature         L       R    A_M Color Spectral_Class  Type\n",
       "0         3068  0.002400  0.1700  16.12   Red              M     0\n",
       "1         3042  0.000500  0.1542  16.60   Red              M     0\n",
       "2         2600  0.000300  0.1020  18.70   Red              M     0\n",
       "3         2800  0.000200  0.1600  16.65   Red              M     0\n",
       "4         1939  0.000138  0.1030  20.06   Red              M     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying first 5 rows\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>**ATTRIBUTES:**\n",
    "**Temperature** -- K,\n",
    "**L** -- L/Lo (Luminosity),\n",
    "**R** -- R/Ro (Radius),\n",
    "**AM** -- Mv (Absolute Magnitude),\n",
    "**Color** -- General Color of Spectrum,\n",
    "**Spectral_Class** -- O,B,A,F,G,K,M / SMASS - https://lweb.cfa.harvard.edu/~pberlind/atlas/htmls/note.html#:~:text=The%20spectral%20types%20and%20sub,%22color%22%20and%20surface%20brightness.,\n",
    "**Type** -- Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants\n",
    "\n",
    "**Type**\n",
    "from 0 to 5\n",
    "\n",
    "Red Dwarf - 0\n",
    "Brown Dwarf - 1\n",
    "White Dwarf - 2\n",
    "Main Sequence - 3\n",
    "Super Giants - 4\n",
    "Hyper Giants - 5\n",
    "\n",
    "Variables:\n",
    "**Type** (The outcome variable 'y')\n",
    "**L (Luminosity), R(Radius)** (predictor variables 'x')</span>\n",
    "\n",
    "*note:* Its easiest if your independent x variables are numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical dependent variable <span style ='background:yellow'>Type</span> has the following categories: **Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5], dtype='int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code that returns the different categories in the y variable.\n",
    "# Different Categories for the Types (Y-variable)\n",
    "my_data[\"Type\"]=pd.Categorical(my_data[\"Type\"])\n",
    "my_data[\"Type\"].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need some training and testing data, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split data in training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = np.array(my_data[\"L\"])\n",
    "y = np.array(my_data[\"Type\"])\n",
    "xRes = x.reshape((-1,1))\n",
    "yRes = y.reshape((-1,1))\n",
    "\n",
    "xRes_train, xRes_test, yRes_train, yRes_test = train_test_split(xRes, yRes, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set, lets try to predict this using our independent variable **<span style ='background:yellow'>Luminosity</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Basic Classification Models\n",
    "\n",
    "In the Jupyter Notebook from lecture 5 a few different Clustering techniques were discussed. Lets explore how these perform on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at some 'real' models, its a good idea to get a baseline in by using one or more of the dummy classifiers. Lets see how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10416666666666667\n",
      "[[ 0  0  0  0 20  0]\n",
      " [ 0  0  0  0 12  0]\n",
      " [ 0  0  0  0 14  0]\n",
      " [ 0  0  0  0 21  0]\n",
      " [ 0  0  0  0 10  0]\n",
      " [ 0  0  0  0 19  0]]\n"
     ]
    }
   ],
   "source": [
    "# code to create, fit and measure the dummy classifiers (see chapter 5.4. in the lecture notebook)\n",
    "# include both the accuracy score and the confusion matrix for each.\n",
    "\n",
    "#Dummy Classifier - Most Frequent\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dumMF = DummyClassifier(strategy='most_frequent')\n",
    "dumMF = dumMF.fit(xRes_train, yRes_train)\n",
    "y_pred = dumMF.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14583333333333334\n",
      "[[1 2 0 4 9 4]\n",
      " [0 0 5 0 5 2]\n",
      " [3 2 5 0 1 3]\n",
      " [1 3 4 1 5 7]\n",
      " [0 1 5 2 1 1]\n",
      " [2 3 3 0 5 6]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier - Stratified\n",
    "dumSrat = DummyClassifier(strategy=\"stratified\")\n",
    "dumSrat = dumSrat.fit(xRes_train, yRes_train)\n",
    "y_pred = dumSrat.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10416666666666667\n",
      "[[ 0  0  0  0 20  0]\n",
      " [ 0  0  0  0 12  0]\n",
      " [ 0  0  0  0 14  0]\n",
      " [ 0  0  0  0 21  0]\n",
      " [ 0  0  0  0 10  0]\n",
      " [ 0  0  0  0 19  0]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier - Prior\n",
    "dumPrior = DummyClassifier(strategy=\"prior\")\n",
    "dumPrior.fit(xRes_train, yRes_train)\n",
    "y_pred = dumPrior.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23958333333333334\n",
      "[[5 4 2 3 1 5]\n",
      " [2 4 3 0 2 1]\n",
      " [1 2 2 1 5 3]\n",
      " [2 4 2 5 5 3]\n",
      " [2 0 2 2 2 2]\n",
      " [1 6 4 3 0 5]]\n"
     ]
    }
   ],
   "source": [
    "#Dummy Classifier - Uniform\n",
    "dumUni = DummyClassifier(strategy=\"uniform\")\n",
    "dumUni.fit(xRes_train, yRes_train)\n",
    "y_pred = dumUni.predict(xRes_test)\n",
    "print(metrics.accuracy_score(yRes_test, y_pred))\n",
    "print(confusion_matrix(yRes_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>There are 4 classifier types. The first one is most frequent. This produces the most frequent label in the training set. The second one is stratified which generates predictions by respecting the training set's class distribution. The third one is prior which always predicts the class that maximizes the class prior. And finally the last one is uniform. This method generates predictions uniformly at random. For each dummy classifier type, there are a accuracy scores to show how accurate the prediction is and the confusion matrixes displays the performance of each classifier. Prior has the same results as most frequent as it looks at the prior results. The \"most frequent's\" confusion matrix displays the prediction of the type of star using luminosity.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, those are our 'baseline'. A model should be able to at least outperform these.\n",
    "\n",
    "Lets dive in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Naive Bayes\n",
    "\n",
    "The first model discussed was the Naive Bayes model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>Naive Bayes is a supervised learning algorithm based on applying the Bayes’ theorem with the assumption of independency between the features or class variables. It works by using Bayes' theorem and is useful for calculating condional probabilities. The formula is: P(A|B)= (P(B|A) x P(A))/P(B), using this formula you can calculate the probability of A based on the probability of B</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create and fit this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model, and fit the data.\n",
    "\n",
    "# Set our model to this GaussianNB\n",
    "gaussNB = GaussianNB()\n",
    "\n",
    "#Get the specific fields of interest\n",
    "mDat =my_data[[\"L\", \"R\", \"Type\"]]\n",
    "\n",
    "# Set input and output\n",
    "X = mDat[[\"L\", \"R\"]]\n",
    "y = mDat[\"Type\"]\n",
    "\n",
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=109)\n",
    "\n",
    "\n",
    "#Create the model and prediction\n",
    "gaussNB.fit(X_train, y_train)\n",
    "y_pred = gaussNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to measure its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  2  0  4  0  0]\n",
      " [ 0  0  0  1  6  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>We got a 56% accuracy rating which shows the accuracy of our predictions on our new model. The numbers which you see on the confusion matrix show the prediction of the luminosity and radius and how many times they were correctly predicted and the ones which predicted it wrong. As you can see, this is a remarkable difference compared to the dummy classifiers. We were mostly getting around the 10% to 20% range most of the time. But now using the naive bayes method, our model got a much more accurate prediction of 56%.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also have a look at what a prediction would be. If the **<span style ='background:yellow'>Luminosity<span>** has a score of **<span style ='background:yellow'>1136</span>** and the **<span style ='background:yellow'>Radius</span>** has a score of **<span style ='background:yellow'>7.2</span>**, then this model will predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  2  0  4  0  0]\n",
      " [ 0  0  0  1  6  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# code to show the prediction\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about it for NB. A nice thing about NB is that it doesn't really require any parameters. Lets look at our next technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines\n",
    "The second model discussed were Support Vector Machines. There is a plural here, because we can use different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>Support vector machines is also another supervised machine learning algorithm that analyses data for predictions for classification. It works by using classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text. </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic kernel is the linear one, so we'll attempt that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model, and fit the data.\n",
    "svmLin = SVC(kernel='linear')\n",
    "svmLin.fit(X_train, y_train)\n",
    "y_pred = svmLin.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring its performance...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "[[ 7  0  2  0  0  0]\n",
      " [ 2  3  0  0  0  0]\n",
      " [ 0  0  9  0  0  0]\n",
      " [ 0  0  0  5  1  0]\n",
      " [ 0  0  0  1  6  0]\n",
      " [ 0  0  0  0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, and the NB?>You might have noticed that the accuracy score is way higher than those of the dummy classifiers as well as the Naive bayes. The dummy classifiers were in the range of 10% to 20%, the naive bayes was 56%. Now the the support vector machine model gives us an accuracy score of 88%, much higher than the rest of them. The numbers you see in the confusion matrix tell you the diagonals and how much seperation there is between them. You can probably see that one diagonal has most of the scores/counts while the others have almost zero counts or scores.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same for the other kernels that were discussed, i.e. rbf, polynomial, and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF results\n",
      "accuracy score:  0.22916666666666666\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  1  0]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  0 12  0]]\n"
     ]
    }
   ],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "\n",
    "#rbf\n",
    "rbfKer = SVC(kernel='rbf')\n",
    "rbfKer.fit(X_train, y_train)\n",
    "y_pred = rbfKer.predict(X_test)\n",
    "\n",
    "print('RBF results')\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial results\n",
      "accuracy score:  0.22916666666666666\n",
      "[[ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  9  0  0  0  0]\n",
      " [ 0  5  0  0  1  0]\n",
      " [ 0  1  0  0  6  0]\n",
      " [ 0  2  0  0 10  0]]\n"
     ]
    }
   ],
   "source": [
    "#Polynomial Kernel\n",
    "svmPol = SVC(kernel='poly')\n",
    "svmPol.fit(X_train, y_train)\n",
    "y_pred = svmPol.predict(X_test)\n",
    "\n",
    "print('Polynomial results')\n",
    "print('accuracy score: ',metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.2916666666666667\n",
      "[[0 9 0 0 0 0]\n",
      " [0 5 0 0 0 0]\n",
      " [0 9 0 0 0 0]\n",
      " [0 5 0 0 0 1]\n",
      " [0 0 0 0 4 3]\n",
      " [0 0 0 0 7 5]]\n"
     ]
    }
   ],
   "source": [
    "#Sigmoid Kernel\n",
    "svmSigm = SVC(kernel='sigmoid')\n",
    "svmSigm.fit(X_train, y_train)\n",
    "y_pred = svmSigm.predict(X_test)\n",
    "\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>We have now used 4 types of support vector machines, linear, RBF aka(Radial Basis Function), polynomial and sigmoid. Out all of them, linear worked best, which means that the data is mostly split in sections where it can be cut by diagonals. RBF and polynomial have similar results of 23% and sigmoid being 29%. All of these methods still perform much better than the dummy classifiers.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, lets move on to the third technique..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. K-Nearest Neighbors\n",
    "The third technique is the K-Nearest Neighbors (KNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. The KNN algorithm works by assuming that similar things exist in close proximity. In other words, similar things are near to each other.</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this we need to do some additional steps.\n",
    "\n",
    "First we need to normalize our x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the StandardScaler to normalize the two x variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# setting the scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Convert the train and test X values, using the same scaler (so based on the X_train)\n",
    "X_trainScaled = scaler.transform(X_train)\n",
    "X_testScaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we need to determine how many neighbors (k) we want. To do this we'll visualize the results using different values for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create the graph with Error Rate vs. K-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the result and your choice for k based on the graph></span>**\n",
    "\n",
    "*Note:* +0.5 if you also use the GridSearch technique to decide on k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we want k to be, we can create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model with the selected k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out how good it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, the NB, and the SVM kernels?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more basic technique to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Decision Trees\n",
    "The last technique that was discussed in detail, were the Decision Trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a Decision Tree method works></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variations were discussed:\n",
    "\n",
    "* ID3 (or entropy with sklearn)\n",
    "* Gini\n",
    "* Random Forest\n",
    "* Extra trees\n",
    "\n",
    "Hopefully we have the hang of this now, so lets do each of them in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, the NB, the SVM kernels, and the knn?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last set of techniques to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Other Models\n",
    "In the Jupyter Notebook from the lecture, in chapter 5.4 a few more techniques were simply shown:\n",
    "\n",
    "* Linear Discriminant Analysis\n",
    "* Quadratic Discriminant Analysis\n",
    "* Logistic Regression Classifier\n",
    "* Multinomial Logistic Regression Classification\n",
    "* Adaptive Boosting\n",
    "* Gradient Boosting\n",
    "* Histogram Gradient Boosting\n",
    "* XGBoost\n",
    "* Stacking\n",
    "\n",
    "Out of curiousity lets see how these perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score (the confusion matrix is here optional).\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<which performed best?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center>-----Chapters 1 and 2 are required to be fully completed to get a 60, the next few chapters will give a +10 for each chapter.<br> \n",
    "    However the template is not as extensive as the previous chapters. <br>\n",
    "    You can select any chapter below the order is not fixed (you can leave the others empty)<br>\n",
    "    You don't have to use the same dataset for the chapters below. If it helps in clarification you can use another dataset, but then make sure to include it as you submit.\n",
    "    ----</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualisation\n",
    "\n",
    "With two input parameters we can actually determine visually where a model will classify a variable into which category. An overview of such plots is shown at https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "We cannot copy that code since it does a comparison. What we want is a function that takes the X and Y data as input, as well as the model to be used and then shows the decision areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Categorical Input\n",
    "With classification we have a categorical output variable, but what if we also have one or more categorical input variables.\n",
    "\n",
    "One popular technique is one-hot-encoding, but there are others.\n",
    "\n",
    "In this chapter we'll discuss **<span style ='background:yellow'>\\<your chosen technique></span>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the technique in detail. What does it do and how does it work></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code of using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Performance\n",
    "Some models get higher accuracy scores than others. In the Jupyter Notebook from the lecture the UFC data was used and the QDA had the highest accuracy score: 0.6747. The big question is, can it be done better? First areas to look for improvement are to simply increase the number of input variables, or tweak some parameters of some of the models, or a combination of both.\n",
    "\n",
    "In this chapter we'll give it an attempt.\n",
    "\n",
    "First we need to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load the UFC data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain your attempt, what did you do.></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that will generate an accuracy score for the outcome that is higher than 0.6747\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. A New Technique\n",
    "\n",
    "Many techniques were discussed in class and the lecture Jupyter Notebook, but there are a lot more. In this chapter the \\<your chosen new technique> is discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain in detail this new technique. Note that other students should be able to understand it from your explanation alone!></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code on using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<feel free to use more cells for this, you probably need them></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
